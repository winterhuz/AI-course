
Neural Network 常常表示成 輸入->計算層->輸出
我們就從一層計算層開始聊起，神經網絡到底是怎麼建構的


Logistic Regression是一個線性回歸的變體分類器  
主要用來分類二元問題  
 
先從數學的角度來看看  
  
當我們輸入一個圖片，想要知道圖片中是否有"貓"  
這個時候，輸入是"圖片" 也就是一個大小依像素而定的多維矩陣  
而輸出是一個介於 0~1 的常數，代表著"是否"的可能性  

這時我們有一組訓練集，每組都有矩陣x輸入以及二元輸出y (0 or 1)

### 1.1 線性函式(linear function)
二元分類器最主要的目的就是劃出一條線分割兩區域
既然是線，自然會想到用一次方程式解

已知輸入 x 是一個矩陣， 輸出 y 是一個常數  
學過矩陣就知道，矩陣直至常數這中間必定經過降維  
我們設計一個矩陣參數 w 以及一個常數參數 b  
  
當 w * x 的時候會使得矩陣降維到 1x1 的大小，再加上參數 b 便是常數  
當然對於n個輸入 w * x 會使得矩陣降維到 nx1的大小，加上 nx1 矩陣參數 b 也會得出 n 個常數  

自此 z = w * x + b

### 1.2 啟動函式(activate function)  
最終得出的常數希望是個機率
因此 z 還要經過啟動函式使其分布在0~1之間  
對於二元分類來說通常指的就是sigmoid函式  
當然還有像是tanh函式、等等  

自此 A = activate(Z)  
  
### 1.3 成本函式(cost function)  
跑完映射函式得出結論後，會執行損失函式(L(A,y))計算與預期的誤差值  
已知希望誤差越小越好，而y只有(0or1)兩種可能，A則必定介於(0,1)之間  

這時我們提出一個滿足y的兩個狀態下皆可以表示誤差的函式
y=1時，A越大就越靠近1，誤差也就越小
y=0時，A越小就越靠近0，誤差也就越小
  
L(A,y) = - (ylogA + (1-y)log(1-A))
  
  
而所有訓練集的損失函式平均值便是成本函式(Cost)。  


神經網絡之所以引人嚮往便是因為最後的"學習"步驟  
這邊的思路是建立一個碗狀的函式，由Cost表示高度，w,b 表示位置  
初始時 w,b 會隨機出現在碗中任意位置，每一次回歸都會令cost值"滑向"底部

w,b值"滑向"底部  
使得cost更趨向底部頂點一點  
  
  

  
  
  


